{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-03T05:45:44.766282Z",
     "iopub.status.busy": "2025-10-03T05:45:44.765728Z",
     "iopub.status.idle": "2025-10-03T05:45:44.775403Z",
     "shell.execute_reply": "2025-10-03T05:45:44.774700Z",
     "shell.execute_reply.started": "2025-10-03T05:45:44.766257Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/project-multi/test_random_400-2.csv\n",
      "/kaggle/input/project-multi/test_top_cosine_200-2.csv\n",
      "/kaggle/input/project-multi/test_top_rougeL_200-2.csv\n",
      "/kaggle/input/project-multi/train_data.csv\n",
      "/kaggle/input/project-multi/test_random_600-2.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T05:45:47.172433Z",
     "iopub.status.busy": "2025-10-03T05:45:47.172155Z",
     "iopub.status.idle": "2025-10-03T05:47:57.436023Z",
     "shell.execute_reply": "2025-10-03T05:47:57.435259Z",
     "shell.execute_reply.started": "2025-10-03T05:45:47.172411Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.44.2\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate==0.34.2\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft==0.11.1\n",
      "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.4.0)\n",
      "Collecting optuna\n",
      "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (0.5.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.44.2)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2) (2.6.0+cu124)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.2)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.5.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.13)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (1.1.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (2025.6.15)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.10.0->accelerate==0.34.2)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.10.0->accelerate==0.34.2)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.10.0->accelerate==0.34.2)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.10.0->accelerate==0.34.2)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.10.0->accelerate==0.34.2)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.10.0->accelerate==0.34.2)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.10.0->accelerate==0.34.2)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.10.0->accelerate==0.34.2)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.10.0->accelerate==0.34.2)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.10.0->accelerate==0.34.2)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.34.2) (1.3.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.34.2) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.44.2) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.44.2) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.44.2) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.44.2) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.44.2) (2024.2.0)\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=c286fda67203d387889f3198ce76c230b63aed8a86baaf24451486cfb1419a21\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, accelerate, rouge_score, peft, optuna, evaluate\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.5.1\n",
      "    Uninstalling fsspec-2025.5.1:\n",
      "      Successfully uninstalled fsspec-2025.5.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.52.4\n",
      "    Uninstalling transformers-4.52.4:\n",
      "      Successfully uninstalled transformers-4.52.4\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.8.1\n",
      "    Uninstalling accelerate-1.8.1:\n",
      "      Successfully uninstalled accelerate-1.8.1\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.15.2\n",
      "    Uninstalling peft-0.15.2:\n",
      "      Successfully uninstalled peft-0.15.2\n",
      "  Attempting uninstall: optuna\n",
      "    Found existing installation: optuna 4.4.0\n",
      "    Uninstalling optuna-4.4.0:\n",
      "      Successfully uninstalled optuna-4.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.34.2 evaluate-0.4.6 fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 optuna-4.5.0 peft-0.11.1 rouge_score-0.1.2 tokenizers-0.19.1 transformers-4.44.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"transformers==4.44.2\" \"accelerate==0.34.2\" \"peft==0.11.1\" optuna evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T05:48:01.037921Z",
     "iopub.status.busy": "2025-10-03T05:48:01.037537Z",
     "iopub.status.idle": "2025-10-03T05:48:01.111570Z",
     "shell.execute_reply": "2025-10-03T05:48:01.110922Z",
     "shell.execute_reply.started": "2025-10-03T05:48:01.037889Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"your_huggingface_token_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T13:02:14.386804Z",
     "iopub.status.busy": "2025-10-02T13:02:14.386102Z",
     "iopub.status.idle": "2025-10-02T15:59:13.892195Z",
     "shell.execute_reply": "2025-10-02T15:59:13.891389Z",
     "shell.execute_reply.started": "2025-10-02T13:02:14.386780Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258f0ccbbc3148cbbe102d773c950a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e011a79d994c4c138887b32c5ba59ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a3ee4f6d374886b56fb0ff18e1ef93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2645ed6c263c4bd5a3d738f68eb80b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/11.4G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model ready.\n",
      "▶️ Running test400, round 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 13:04:30.493682: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759410270.885328      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759410270.995466      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done test400 run 1: wrote 400 rows → ./t5_infer/pred_test400_run1.csv\n",
      "▶️ Running test600, round 1 ...\n",
      "✅ Done test600 run 1: wrote 600 rows → ./t5_infer/pred_test600_run1.csv\n",
      "▶️ Running test400, round 2 ...\n",
      "✅ Done test400 run 2: wrote 400 rows → ./t5_infer/pred_test400_run2.csv\n",
      "▶️ Running test600, round 2 ...\n",
      "✅ Done test600 run 2: wrote 600 rows → ./t5_infer/pred_test600_run2.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os, time, json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# ========= CONFIG =========\n",
    "MODEL_NAME     = \"google-t5/t5-3b\"\n",
    "INPUT_FILES    = {\n",
    "    \"test400\": \"/kaggle/input/project-multi/test_random_400-2.csv\",\n",
    "    \"test600\": \"/kaggle/input/project-multi/test_random_600-2.csv\",\n",
    "}\n",
    "INPUT_COL      = \"description_html_clean\"   \n",
    "OUTPUT_DIR     = \"./t5_infer\"\n",
    "MAX_SOURCE_LEN = 512\n",
    "\n",
    "GEN_KWARGS = dict(\n",
    "    max_new_tokens=64,      \n",
    "    num_beams=4,\n",
    "    do_sample=False,\n",
    "    no_repeat_ngram_size=3,\n",
    "    length_penalty=1.0,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "\n",
    "USE_APP_PROMPT = False\n",
    "def build_prompt_for_app(html_text: str) -> str:\n",
    "    return (\n",
    "        \"summarize: You are an expert app store editor. \"\n",
    "        \"Given the following app description in HTML format, summarize it in 2-3 sentences, \"\n",
    "        \"with a concise, engaging short description (max 80 characters) suitable for an app store listing. \"\n",
    "        f\"App Description HTML:\\n{html_text}\\n\"\n",
    "        \"Format your response as:\\n\"\n",
    "        \"Short Description: <your short description>\\n\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"⏳ Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"✅ Model ready.\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "RUN_SCHEDULE = [\"test400\", \"test600\", \"test400\", \"test600\"]\n",
    "run_counters = {k: 0 for k in INPUT_FILES.keys()}\n",
    "\n",
    "for tag in RUN_SCHEDULE:\n",
    "    run_counters[tag] += 1\n",
    "    run_id = run_counters[tag]\n",
    "\n",
    "    file_path = INPUT_FILES[tag]\n",
    "    assert os.path.exists(file_path), f\"Missing: {file_path}\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    assert INPUT_COL in df.columns, f\"Missing column: {INPUT_COL}\"\n",
    "\n",
    "    print(f\"▶️ Running {tag}, round {run_id} ...\")\n",
    "    pred_rows = []\n",
    "    infer_log_path = os.path.join(OUTPUT_DIR, f\"log_{tag}_run{run_id}.jsonl\")\n",
    "    pred_path      = os.path.join(OUTPUT_DIR, f\"pred_{tag}_run{run_id}.csv\")\n",
    "\n",
    "    with open(infer_log_path, \"w\", encoding=\"utf-8\") as f_log:\n",
    "        for i, row in df.iterrows():\n",
    "            raw_src = str(row[INPUT_COL])\n",
    "            src_text = build_prompt_for_app(raw_src) if USE_APP_PROMPT else raw_src\n",
    "\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                src_text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=MAX_SOURCE_LEN,\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "           \n",
    "            input_tokens = int(inputs[\"attention_mask\"].sum().item())\n",
    "\n",
    "            \n",
    "            t0 = time.time()\n",
    "            with torch.inference_mode():\n",
    "                outputs = model.generate(**inputs, **GEN_KWARGS)\n",
    "            latency = time.time() - t0\n",
    "\n",
    "            \n",
    "            decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "       \n",
    "            gen_tokens = int((outputs[0] != tokenizer.pad_token_id).sum().item())\n",
    "\n",
    "       \n",
    "            f_log.write(json.dumps({\n",
    "                \"index\": i,\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"gen_tokens\": gen_tokens,\n",
    "                \"latency_sec\": round(latency, 4),\n",
    "            }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            pred_rows.append({\"index\": i, \"prediction\": decoded})\n",
    "\n",
    "    pd.DataFrame(pred_rows).to_csv(pred_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Done {tag} run {run_id}: wrote {len(pred_rows)} rows → {pred_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peagsus xsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T08:04:47.333316Z",
     "iopub.status.busy": "2025-10-01T08:04:47.333027Z",
     "iopub.status.idle": "2025-10-01T08:11:16.431733Z",
     "shell.execute_reply": "2025-10-01T08:11:16.430928Z",
     "shell.execute_reply.started": "2025-10-01T08:04:47.333295Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ad68f065264c8db7bbfd43b388f76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0598f1cb06c7479bba39c62ca5660069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7297c8bf87e42ebaf8af17e8e978b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ec0bf0e3154d7aab77c5ad9c455360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e86595bb4384758bb03272ed0178a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e34085f69d647c2b82f36e399faa0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/260 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to: ./pegasus_xsum_infer/pegasus_xsum_predictions.csv\n",
      "Inference token/time logs: ./pegasus_xsum_infer/inference_token_time.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, time, json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# ========= CONFIG =========\n",
    "MODEL_NAME    = \"google/pegasus-large\"\n",
    "INPUT_CSV     = \"/kaggle/input/project-multi/test_random_400-2.csv\" \n",
    "INPUT_COL     = \"description_html_clean\"   \n",
    "OUTPUT_DIR    = \"./pegasus_xsum_infer\"\n",
    "PRED_CSV      = \"pegasus_xsum_predictions.csv\"\n",
    "INFER_LOG     = \"inference_token_time.jsonl\"\n",
    "USE_APP_PROMPT = False      \n",
    "MAX_SOURCE_LEN = 512\n",
    "\n",
    "\n",
    "GEN_KWARGS = dict(\n",
    "    max_new_tokens=64,\n",
    "    num_beams=4,\n",
    "    do_sample=False,\n",
    "    no_repeat_ngram_size=3,\n",
    "    length_penalty=1.0,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "\n",
    "def build_prompt_for_app(html_text: str) -> str:\n",
    "    return (\n",
    "        \"You are an expert app store editor. \"\n",
    "        \"Given the following app description in HTML format, summarize it in 2-3 sentences, \"\n",
    "        \"with a concise, engaging short description (max 80 characters) suitable for an app store listing. \"\n",
    "        f\"App Description HTML:\\n{html_text}\\n\"\n",
    "        \"Format your response as:\\n\"\n",
    "        \"Short Description: <your short description>\\n\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "assert os.path.exists(INPUT_CSV), f\"Missing: {INPUT_CSV}\"\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "assert INPUT_COL in df.columns, f\"Missing column: {INPUT_COL}\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "infer_log_path = os.path.join(OUTPUT_DIR, INFER_LOG)\n",
    "pred_path      = os.path.join(OUTPUT_DIR, PRED_CSV)\n",
    "\n",
    "# ========= LOAD MODEL =========\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "pred_rows = []\n",
    "with open(infer_log_path, \"w\", encoding=\"utf-8\") as f_log:\n",
    "    for i, row in df.iterrows():\n",
    "        raw_src = str(row[INPUT_COL])\n",
    "\n",
    "        src_text = build_prompt_for_app(raw_src) if USE_APP_PROMPT else raw_src\n",
    "\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            src_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_SOURCE_LEN,\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        \n",
    "        input_tokens = int(inputs[\"attention_mask\"].sum().item()) if \"attention_mask\" in inputs else int((inputs[\"input_ids\"] != tokenizer.pad_token_id).sum().item())\n",
    "\n",
    "        \n",
    "        t0 = time.time()\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(**inputs, **GEN_KWARGS)\n",
    "        latency = time.time() - t0\n",
    "\n",
    "        text = tokenizer.decode(out[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "        \n",
    "        gen_ids = out[0]\n",
    "        try:\n",
    "            output_tokens = int((gen_ids != tokenizer.pad_token_id).sum().item())\n",
    "        except Exception:\n",
    "            output_tokens = int(len(gen_ids))\n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        tps = total_tokens / latency if latency > 0 else None\n",
    "\n",
    "\n",
    "        pred_rows.append({\"index\": i, \"prediction\": text})\n",
    "\n",
    "\n",
    "        f_log.write(json.dumps({\n",
    "            \"event\": \"inference\",\n",
    "            \"row_index\": int(i),\n",
    "            \"input_tokens\": int(input_tokens),\n",
    "            \"output_tokens\": int(output_tokens),\n",
    "            \"total_tokens\": int(total_tokens),\n",
    "            \"latency_sec\": float(latency),\n",
    "            \"tokens_per_sec\": float(tps) if tps is not None else None,\n",
    "            \"gen_kwargs\": GEN_KWARGS,\n",
    "            \"timestamp\": time.time(),\n",
    "        }) + \"\\n\")\n",
    "\n",
    "\n",
    "pd.DataFrame(pred_rows).to_csv(pred_path, index=False, encoding=\"utf-8\")\n",
    "print(\"Saved predictions to:\", pred_path)\n",
    "print(\"Inference token/time logs:\", infer_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T05:48:11.888958Z",
     "iopub.status.busy": "2025-10-03T05:48:11.888382Z",
     "iopub.status.idle": "2025-10-03T05:55:04.508991Z",
     "shell.execute_reply": "2025-10-03T05:55:04.508261Z",
     "shell.execute_reply.started": "2025-10-03T05:48:11.888934Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      " Loading model: google/pegasus-xsum (pegasus-xsum)\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model ready.\n",
      "▶️ [pegasus-xsum] Running test400, round 1 ...\n",
      "✅ [pegasus-xsum] Done test400 run 1: 400 rows → ./pegasus_infer_schedule/pegasus-xsum/pred_test400_run1.csv\n",
      "▶️ [pegasus-xsum] Running test400, round 2 ...\n",
      "✅ [pegasus-xsum] Done test400 run 2: 400 rows → ./pegasus_infer_schedule/pegasus-xsum/pred_test400_run2.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os, time, json, gc\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# ========= CONFIG =========\n",
    "MODELS = {\n",
    "    #\"pegasus-large\":       \"google/pegasus-large\",\n",
    "    \"pegasus-xsum\": \"google/pegasus-xsum\",\n",
    "}\n",
    "INPUT_FILES = {\n",
    "    \"test400\": \"/kaggle/input/project-multi/test_random_400-2.csv\",\n",
    "    \"test600\": \"/kaggle/input/project-multi/test_random_600-2.csv\",\n",
    "}\n",
    "RUN_SCHEDULE = [\"test400\",\"test400\"]\n",
    "\n",
    "INPUT_COL      = \"description_html_clean\"\n",
    "OUTPUT_ROOT    = \"./pegasus_infer_schedule\"\n",
    "MAX_SOURCE_LEN = 512\n",
    "\n",
    "GEN_KWARGS = dict(\n",
    "    max_new_tokens=64,\n",
    "    num_beams=4,\n",
    "    do_sample=False,\n",
    "    no_repeat_ngram_size=3,\n",
    "    length_penalty=1.0,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "USE_APP_PROMPT = False\n",
    "def build_prompt_for_app(html_text: str) -> str:\n",
    "    return (\n",
    "        \"You are an expert app store editor. \"\n",
    "        \"Given the following app description in HTML format, summarize it in 2-3 sentences, \"\n",
    "        \"with a concise, engaging short description (max 80 characters) suitable for an app store listing. \"\n",
    "        f\"App Description HTML:\\n{html_text}\\n\"\n",
    "        \"Format your response as:\\n\"\n",
    "        \"Short Description: <your short description>\\n\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for model_tag, model_name in MODELS.items():\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\" Loading model: {model_name} ({model_tag})\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    OUT_DIR = os.path.join(OUTPUT_ROOT, model_tag)\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    model.eval().to(device)\n",
    "    print(\"Model ready.\")\n",
    "\n",
    "    run_counters = {\"test400\": 0, \"test600\": 0}\n",
    "\n",
    "    for tag in RUN_SCHEDULE:\n",
    "        run_counters[tag] += 1\n",
    "        run_id = run_counters[tag]\n",
    "\n",
    "        file_path = INPUT_FILES[tag]\n",
    "        assert os.path.exists(file_path), f\"Missing: {file_path}\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        assert INPUT_COL in df.columns, f\"Missing column: {INPUT_COL}\"\n",
    "\n",
    "        print(f\"▶️ [{model_tag}] Running {tag}, round {run_id} ...\")\n",
    "        pred_rows = []\n",
    "        infer_log_path = os.path.join(OUT_DIR, f\"log_{tag}_run{run_id}.jsonl\")\n",
    "        pred_path      = os.path.join(OUT_DIR, f\"pred_{tag}_run{run_id}.csv\")\n",
    "\n",
    "        with open(infer_log_path, \"w\", encoding=\"utf-8\") as f_log:\n",
    "            for i, row in df.iterrows():\n",
    "                raw_src = str(row[INPUT_COL])\n",
    "                src_text = build_prompt_for_app(raw_src) if USE_APP_PROMPT else raw_src\n",
    "\n",
    "             \n",
    "                inputs = tokenizer(\n",
    "                    src_text,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    max_length=MAX_SOURCE_LEN,\n",
    "                )\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                input_tokens = int(inputs[\"attention_mask\"].sum().item())\n",
    "\n",
    "             \n",
    "                t0 = time.time()\n",
    "                with torch.no_grad():\n",
    "                    out = model.generate(**inputs, **GEN_KWARGS)\n",
    "                latency = time.time() - t0\n",
    "\n",
    "              \n",
    "                text = tokenizer.decode(out[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "                gen_ids = out[0]\n",
    "                try:\n",
    "                    output_tokens = int((gen_ids != tokenizer.pad_token_id).sum().item())\n",
    "                except Exception:\n",
    "                    output_tokens = int(len(gen_ids))\n",
    "                total_tokens = input_tokens + output_tokens\n",
    "                tps = total_tokens / latency if latency > 0 else None\n",
    "\n",
    "                pred_rows.append({\"index\": i, \"prediction\": text})\n",
    "\n",
    "                f_log.write(json.dumps({\n",
    "                    \"event\": \"inference\",\n",
    "                    \"row_index\": int(i),\n",
    "                    \"input_tokens\": input_tokens,\n",
    "                    \"output_tokens\": output_tokens,\n",
    "                    \"total_tokens\": total_tokens,\n",
    "                    \"latency_sec\": float(latency),\n",
    "                    \"tokens_per_sec\": float(tps) if tps else None,\n",
    "                    \"gen_kwargs\": GEN_KWARGS,\n",
    "                    \"timestamp\": time.time(),\n",
    "                }) + \"\\n\")\n",
    "\n",
    "        pd.DataFrame(pred_rows).to_csv(pred_path, index=False, encoding=\"utf-8\")\n",
    "        print(f\"[{model_tag}] Done {tag} run {run_id}: {len(pred_rows)} rows → {pred_path}\")\n",
    "\n",
    "\n",
    "    del model\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T05:13:54.681333Z",
     "iopub.status.busy": "2025-10-01T05:13:54.681060Z",
     "iopub.status.idle": "2025-10-01T05:13:55.048963Z",
     "shell.execute_reply": "2025-10-01T05:13:55.048251Z",
     "shell.execute_reply.started": "2025-10-01T05:13:54.681314Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!zip -r -q /kaggle/working//pegasus_xsum_infer.zip /kaggle/working/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T08:19:32.967144Z",
     "iopub.status.busy": "2025-10-01T08:19:32.966177Z",
     "iopub.status.idle": "2025-10-01T08:24:14.158186Z",
     "shell.execute_reply": "2025-10-01T08:24:14.157378Z",
     "shell.execute_reply.started": "2025-10-01T08:19:32.967114Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Loading base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model ready on cuda\n",
      "🎯 Done. Saved predictions to: bart_predictions_zero_shot.csv\n",
      "🗒  Logs at: inference_logs_zero_shot.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, time, csv\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# ----------- CONFIG -----------\n",
    "TEST_CSV = \"/kaggle/input/project-multi/test_random_400-2.csv\"\n",
    "MODEL_NAME = \"facebook/bart-large\"      \n",
    "PRED_CSV = \"bart_predictions_zero_shot.csv\"\n",
    "INFER_LOG_CSV = \"inference_logs_zero_shot.csv\"\n",
    "MAX_SOURCE_LEN = 1000                  \n",
    "\n",
    "GEN_KW = dict(\n",
    "    max_new_tokens=64,                \n",
    "    num_beams=4,                       \n",
    "    no_repeat_ngram_size=3,            \n",
    "    length_penalty=2.0,                 \n",
    "    early_stopping=True,\n",
    "    do_sample=False,                    \n",
    ")\n",
    "\n",
    "\n",
    "assert os.path.exists(TEST_CSV), f\"Missing: {TEST_CSV}\"\n",
    "df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "TEST_INPUT_COL = None\n",
    "for cand in [\"description_html_clean\", \"description_html\"]:\n",
    "    if cand in df.columns:\n",
    "        TEST_INPUT_COL = cand\n",
    "        break\n",
    "assert TEST_INPUT_COL is not None, \"Test CSV cần 'description_html_clean' hoặc 'description_html'\"\n",
    "\n",
    "print(\"Loading base model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(\"Model ready on\", device)\n",
    "\n",
    "\n",
    "with open(INFER_LOG_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    csv.writer(f).writerow([\"index\",\"input_tokens\",\"gen_tokens\",\"latency_sec\"])\n",
    "\n",
    "@torch.inference_mode()\n",
    "def infer_one(html: str, idx: int) -> str:\n",
    "\n",
    "    src = str(html)\n",
    "    enc = tokenizer(\n",
    "        src,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_SOURCE_LEN,\n",
    "    )\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "    t0 = time.time()\n",
    "    out = model.generate(**enc, **GEN_KW)\n",
    "    dur = time.time() - t0\n",
    "\n",
    "    # log\n",
    "    input_tokens = int(enc[\"attention_mask\"].sum().item())\n",
    "    gen_tokens = int(out.shape[1] - enc[\"input_ids\"].shape[1])\n",
    "    with open(INFER_LOG_CSV, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        csv.writer(f).writerow([idx, input_tokens, gen_tokens, f\"{dur:.4f}\"])\n",
    "\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True, clean_up_tokenization_spaces=True).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "preds = []\n",
    "for i, html in enumerate(df[TEST_INPUT_COL].astype(str).tolist()):\n",
    "    preds.append(infer_one(html, i))\n",
    "\n",
    "df[\"bart_pred\"] = preds\n",
    "df.to_csv(PRED_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"Done. Saved predictions to: {PRED_CSV}\")\n",
    "print(f\"Logs at: {INFER_LOG_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T04:21:33.806197Z",
     "iopub.status.busy": "2025-10-02T04:21:33.805827Z",
     "iopub.status.idle": "2025-10-02T04:44:59.255812Z",
     "shell.execute_reply": "2025-10-02T04:44:59.255023Z",
     "shell.execute_reply.started": "2025-10-02T04:21:33.806165Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏳ Loading facebook/bart-large | test400 (run 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e55d2d9c15140aab0e9e7882a9384a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9178caa82704bd5a2a5535156500005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2080bd68739b4f5ebfda52191454b1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8984732e63c14ac4b3e37cbe53961738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73ab61b77ed4e7da108fea4961891d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0231b1823b4216a555a1e53dba869f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model ready on cuda\n",
      "🎯 Saved predictions → bart-base_test400_run1_pred.csv\n",
      "🗒  Logs → bart-base_test400_run1_log.csv\n",
      "\n",
      "⏳ Loading facebook/bart-large | test600 (run 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model ready on cuda\n",
      "🎯 Saved predictions → bart-base_test600_run1_pred.csv\n",
      "🗒  Logs → bart-base_test600_run1_log.csv\n",
      "\n",
      "⏳ Loading facebook/bart-large | test400 (run 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model ready on cuda\n",
      "🎯 Saved predictions → bart-base_test400_run2_pred.csv\n",
      "🗒  Logs → bart-base_test400_run2_log.csv\n",
      "\n",
      "⏳ Loading facebook/bart-large | test600 (run 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model ready on cuda\n",
      "🎯 Saved predictions → bart-base_test600_run2_pred.csv\n",
      "🗒  Logs → bart-base_test600_run2_log.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, time, csv\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# ===== CONFIG =====\n",
    "INPUT_FILES = {\n",
    "    \"test400\": \"/kaggle/input/project-multi/test_random_400-2.csv\",\n",
    "    \"test600\": \"/kaggle/input/project-multi/test_random_600-2.csv\",\n",
    "}\n",
    "RUN_SCHEDULE = [\"test400\", \"test600\", \"test400\", \"test600\"] \n",
    "MODELS = {\n",
    "    \"bart-base\": \"facebook/bart-large\",\n",
    "    #\"bart-xsum\": \"facebook/bart-large-xsum\",\n",
    "}\n",
    "\n",
    "MAX_SOURCE_LEN = 1000\n",
    "GEN_KW = dict(\n",
    "    max_new_tokens=64,     \n",
    "    num_beams=4,\n",
    "    no_repeat_ngram_size=3,\n",
    "    length_penalty=2.0,\n",
    "    early_stopping=True,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "INPUT_COL_CANDIDATES = [\"description_html_clean\", \"description_html\"]\n",
    "\n",
    "def run_one_round(model_tag, model_name, test_tag, run_id):\n",
    "    test_file = INPUT_FILES[test_tag]\n",
    "    assert os.path.exists(test_file), f\"Missing: {test_file}\"\n",
    "    df = pd.read_csv(test_file)\n",
    "\n",
    "    test_input_col = None\n",
    "    for c in INPUT_COL_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            test_input_col = c\n",
    "            break\n",
    "    assert test_input_col is not None, f\"CSV {test_file} cần 1 trong các cột {INPUT_COL_CANDIDATES}\"\n",
    "\n",
    "    pred_csv = f\"{model_tag}_{test_tag}_run{run_id}_pred.csv\"\n",
    "    log_csv  = f\"{model_tag}_{test_tag}_run{run_id}_log.csv\"\n",
    "\n",
    "    print(f\"\\n Loading {model_name} | {test_tag} (run {run_id})\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    print(f\"Model ready on {device}\")\n",
    "\n",
    "    with open(log_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        csv.writer(f).writerow([\"index\", \"input_tokens\", \"gen_tokens\", \"latency_sec\"])\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def infer_one(text: str, idx: int) -> str:\n",
    "        enc = tokenizer(\n",
    "            str(text),\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_SOURCE_LEN,\n",
    "        )\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "        t0 = time.time()\n",
    "        out = model.generate(**enc, **GEN_KW)\n",
    "        dur = time.time() - t0\n",
    "\n",
    "        # input tokens\n",
    "        input_tokens = int(enc[\"attention_mask\"].sum().item())\n",
    "\n",
    "        # output tokens (seq2seq generate trả riêng decoder output)\n",
    "        gen_tokens = int((out[0] != tokenizer.pad_token_id).sum().item())\n",
    "\n",
    "        with open(log_csv, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            csv.writer(f).writerow([idx, input_tokens, gen_tokens, f\"{dur:.4f}\"])\n",
    "\n",
    "        return tokenizer.decode(out[0], skip_special_tokens=True, clean_up_tokenization_spaces=True).strip()\n",
    "\n",
    "    preds = []\n",
    "    for i, src in enumerate(df[test_input_col].astype(str).tolist()):\n",
    "        preds.append(infer_one(src, i))\n",
    "\n",
    "    df[f\"{model_tag}_pred\"] = preds\n",
    "    df.to_csv(pred_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved predictions → {pred_csv}\")\n",
    "    print(f\"Logs → {log_csv}\")\n",
    "\n",
    "# ===== MAIN (interleaved schedule) =====\n",
    "if __name__ == \"__main__\":\n",
    "    for model_tag, model_name in MODELS.items():\n",
    "       \n",
    "        counters = {\"test400\": 0, \"test600\": 0}\n",
    "        for test_tag in RUN_SCHEDULE:\n",
    "            counters[test_tag] += 1\n",
    "            run_one_round(model_tag, model_name, test_tag, counters[test_tag])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8005302,
     "sourceId": 12693174,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
