{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-26T16:56:48.099913Z",
     "iopub.status.busy": "2025-08-26T16:56:48.099410Z",
     "iopub.status.idle": "2025-08-26T16:56:50.975901Z",
     "shell.execute_reply": "2025-08-26T16:56:50.975292Z",
     "shell.execute_reply.started": "2025-08-26T16:56:48.099893Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/project-multi/test_random_400-2.csv\n",
      "/kaggle/input/project-multi/test_top_cosine_200-2.csv\n",
      "/kaggle/input/project-multi/test_top_rougeL_200-2.csv\n",
      "/kaggle/input/project-multi/train_data.csv\n",
      "/kaggle/input/project-multi/test_random_600-2.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T16:56:50.977527Z",
     "iopub.status.busy": "2025-08-26T16:56:50.977251Z",
     "iopub.status.idle": "2025-08-26T16:58:33.275065Z",
     "shell.execute_reply": "2025-08-26T16:58:33.274327Z",
     "shell.execute_reply.started": "2025-08-26T16:56:50.977511Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.9/504.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U transformers accelerate datasets peft bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T16:58:33.276246Z",
     "iopub.status.busy": "2025-08-26T16:58:33.276037Z",
     "iopub.status.idle": "2025-08-26T16:58:33.796469Z",
     "shell.execute_reply": "2025-08-26T16:58:33.795737Z",
     "shell.execute_reply.started": "2025-08-26T16:58:33.276224Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"your_token_here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T16:58:33.797665Z",
     "iopub.status.busy": "2025-08-26T16:58:33.797412Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 16:58:53.375791: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756227533.750038      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756227533.856072      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad86304f2c98440b9b8afca1a379d201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07bfe4b3ebfe475b878b5c9eff4e80b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a62d9380d0b4fbd80492dc6b3643458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dab131511cb4beca8cbc93a06c8ecdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1365891c467d4594933639818aed9cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49768465596b46f4b19ef224d2333b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f54575ba394350a3e91739f2090eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf3473ae1e85471ea90e3882f8525234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e1be5b03cd4fd48045cb3e9442b216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f86ae8f48647998e5711f27474b6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf042261d8c4767bd7a5247bc3a7b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 01:20, Epoch 2.59/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import gc\n",
    "import random\n",
    "import os, json, torch\n",
    "import pandas as pd\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "if hasattr(torch.backends, \"cuda\"):\n",
    "    torch.backends.cuda.enable_flash_sdp(False)\n",
    "    torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "    torch.backends.cuda.enable_math_sdp(True)\n",
    "\n",
    "\n",
    "TRAIN_CSV = \"/kaggle/input/project-multi/train_data.csv\"\n",
    "TEST_CSV  = \"/kaggle/input/project-multi/test_random_400-2.csv\"\n",
    "\n",
    "assert os.path.exists(TRAIN_CSV), f\"Missing TRAIN_CSV: {TRAIN_CSV}\"\n",
    "assert os.path.exists(TEST_CSV),  f\"Missing TEST_CSV: {TEST_CSV}\"\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "INPUT_COL = \"description_html_clean\" if \"description_html_clean\" in train_df.columns else \\\n",
    "            (\"description_html\" if \"description_html\" in train_df.columns else None)\n",
    "assert INPUT_COL is not None, \"Train CSV needs 'description_html_clean' or 'description_html'\"\n",
    "\n",
    "TARGET_COL = \"description_short\"\n",
    "assert TARGET_COL in train_df.columns, \"Train CSV needs 'description_short'\"\n",
    "\n",
    "def build_prompt(description_html: str) -> str:\n",
    "    return (\n",
    "        \"You are an expert app store editor. \"\n",
    "        \"Given the following app description in HTML format, summarize it in 2-3 sentences, \"\n",
    "        \"with a concise, engaging short description (max 80 characters) suitable for an app store listing. \"\n",
    "        f\"App Description HTML:\\n{description_html}\\n\"\n",
    "        \"Format your response as:\\n\"\n",
    "        \"Short Description: <your short description>\\n\\n\"\n",
    "    )\n",
    "\n",
    "records = []\n",
    "for _, row in train_df.iterrows():\n",
    "    html = str(row[INPUT_COL])\n",
    "    target = str(row[TARGET_COL]).strip()\n",
    "    records.append({\"prompt\": build_prompt(html), \"response\": target})\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(records)\n",
    "split_idx = max(1, int(0.9 * len(records))) if len(records) > 1 else 1\n",
    "train_items = records[:split_idx]\n",
    "dev_items   = records[split_idx:] if split_idx < len(records) else records[:1]\n",
    "\n",
    "\n",
    "class PromptDataset(Dataset):\n",
    "    def __init__(self, items, tokenizer, max_len=2048):\n",
    "        self.items = items\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.items[idx]\n",
    "        prompt_ids   = self.tok(ex[\"prompt\"], add_special_tokens=False, truncation=True, max_length=self.max_len)[\"input_ids\"]\n",
    "        # + eos\n",
    "        response_ids = self.tok(ex[\"response\"] + self.tok.eos_token, add_special_tokens=False, truncation=True, max_length=self.max_len)[\"input_ids\"]\n",
    "        input_ids = prompt_ids + response_ids\n",
    "        labels = [-100]*len(prompt_ids) + response_ids\n",
    "\n",
    "        # truncate to max_len from the right\n",
    "        if len(input_ids) > self.max_len:\n",
    "            input_ids = input_ids[-self.max_len:]\n",
    "            labels    = labels[-self.max_len:]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "            \"attention_mask\": torch.ones(len(input_ids), dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    input_ids = [b[\"input_ids\"] for b in batch]\n",
    "    labels = [b[\"labels\"] for b in batch]\n",
    "    attn = [b[\"attention_mask\"] for b in batch]\n",
    "\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=pad_id)\n",
    "    labels_padded    = pad_sequence(labels,    batch_first=True, padding_value=-100)\n",
    "    attn_padded      = pad_sequence(attn,      batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\"input_ids\": input_ids_padded, \"labels\": labels_padded, \"attention_mask\": attn_padded}\n",
    "\n",
    "\n",
    "MODEL_NAME = \"google/gemma-2-2b-it\"\n",
    "\n",
    "gpu_ok = torch.cuda.is_available()\n",
    "bf16_ok = gpu_ok and (torch.cuda.get_device_capability(0)[0] >= 8)\n",
    "compute_dtype = torch.bfloat16 if bf16_ok else torch.float16\n",
    "device_index = torch.cuda.current_device() if gpu_ok else 0\n",
    "device_map = {\"\": device_index}\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map=device_map,\n",
    "    attn_implementation=\"eager\", \n",
    ")\n",
    "\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.1, bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.config.use_cache = False\n",
    "\n",
    "train_ds = PromptDataset(train_items, tokenizer, max_len=2048)\n",
    "dev_ds   = PromptDataset(dev_items, tokenizer, max_len=2048)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./gemma2_adapter\",\n",
    "    per_device_train_batch_size=1,     \n",
    "    gradient_accumulation_steps=16,    \n",
    "    num_train_epochs=3,               \n",
    "    learning_rate=1e-4,\n",
    "    fp16=not bf16_ok,\n",
    "    bf16=bf16_ok,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=dev_ds,                \n",
    "    data_collator=collate_fn,          \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "os.makedirs(\"./gemma2_adapter\", exist_ok=True)\n",
    "model.save_pretrained(\"./gemma2_adapter\")\n",
    "tokenizer.save_pretrained(\"./gemma2_adapter\")\n",
    "\n",
    "del trainer, model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "base_for_infer = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map=device_map,\n",
    "    attn_implementation=\"eager\",  \n",
    ")\n",
    "model_inf = PeftModel.from_pretrained(base_for_infer, \"./gemma2_adapter\")\n",
    "model_inf.eval()\n",
    "\n",
    "TEST_INPUT_COL = \"description_html_clean\" if \"description_html_clean\" in test_df.columns else \\\n",
    "                 (\"description_html\" if \"description_html\" in test_df.columns else None)\n",
    "assert TEST_INPUT_COL is not None, \"Test CSV needs 'description_html_clean' or 'description_html'\"\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_short(description_html: str, max_new_tokens: int = 64) -> str:\n",
    "    prompt = build_prompt(description_html)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_inf.device)\n",
    "    out = model_inf.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    generated_tokens = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    return text.strip()\n",
    "\n",
    "test_df[\"pred_short_description\"] = [\n",
    "    generate_short(str(x)) for x in test_df[TEST_INPUT_COL].astype(str).tolist()\n",
    "]\n",
    "\n",
    "OUT_CSV = \"predictions.csv\"\n",
    "test_df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Done. Saved: {OUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "train_texts = train_df[INPUT_COL].astype(str).tolist()\n",
    "test_texts  = test_df[TEST_INPUT_COL].astype(str).tolist()\n",
    "\n",
    "\n",
    "train_set = set(train_texts)\n",
    "test_set  = set(test_texts)\n",
    "exact_overlap = train_set.intersection(test_set)\n",
    "\n",
    "print(f\"Số lượng test samples: {len(test_texts)}\")\n",
    "print(f\"Số lượng test trùng EXACT với train: {len(exact_overlap)}\")\n",
    "print(f\"Tỉ lệ exact overlap: {len(exact_overlap)/len(test_texts):.2%}\")\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000).fit(train_texts + test_texts)\n",
    "train_vecs = vectorizer.transform(train_texts)\n",
    "test_vecs  = vectorizer.transform(test_texts)\n",
    "\n",
    "threshold = 0.8 \n",
    "similar_count = 0\n",
    "for i, test_vec in enumerate(test_vecs):\n",
    "    sims = cosine_similarity(test_vec, train_vecs).flatten()\n",
    "    if sims.max() >= threshold:\n",
    "        similar_count += 1\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8005302,
     "sourceId": 12693174,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
